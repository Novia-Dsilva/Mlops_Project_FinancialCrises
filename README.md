# Financial Crisis Detection - MLOps Pipeline

	â *A production-ready MLOps pipeline for financial stress testing using dual-model architecture (VAE for scenario generation + XGBoost/LSTM for prediction)*

[![Coverage](https://img.shields.io/badge/coverage-84%25-brightgreen.svg)](htmlcov/index.html)
[![Python](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

---

## ğŸ“‹ Table of Contents

â€¢â   â [Overview](#overview)
â€¢â   â [Architecture](#architecture)
â€¢â   â [Project Structure](#project-structure)
â€¢â   â [Prerequisites](#prerequisites)
â€¢â   â [Installation](#installation)
â€¢â   â [Configuration](#configuration)
â€¢â   â [Pipeline Execution](#pipeline-execution)
â€¢â   â [Data Validation](#data-validation)
â€¢â   â [Testing](#testing)
â€¢â   â [Monitoring & Alerts](#monitoring--alerts)
â€¢â   â [Reproducibility](#reproducibility)
â€¢â   â [Troubleshooting](#troubleshooting)

---

## ğŸ¯ Overview

This MLOps pipeline implements a comprehensive data processing workflow for financial stress testing with:

â€¢â   â *Point-in-time correctness* - 45-day reporting lag for quarterly financials
â€¢â   â *Dual-pipeline architecture* - VAE for scenarios + XGBoost/LSTM for predictions
â€¢â   â *Comprehensive validation* - 4 checkpoints with Great Expectations
â€¢â   â *Data versioning* - DVC for reproducibility
â€¢â   â *Quality assurance* - Anomaly detection, bias detection, drift detection
â€¢â   â *Production-ready* - Airflow orchestration, monitoring, alerting
â€¢â   â *Test coverage* - 84% (exceeds 75% requirement)

### *Data Sources:*
â€¢â   â *FRED* - 13 macroeconomic indicators (GDP, CPI, unemployment, etc.)
â€¢â   â *Yahoo Finance* - Market data (VIX, S&P 500) + 25 company stock prices
â€¢â   â *Alpha Vantage* - Company fundamentals (quarterly income statements & balance sheets)

### *Time Period:*
â€¢â   â *2005-01-01 to Present* (~20 years, covering 2008 crisis and 2020 COVID)

---

## ğŸ—ï¸ Architecture

### *Pipeline Flow:*


Data Collection â†’ Validate Raw â†’ Clean â†’ Validate Clean â†’ 
Feature Engineering â†’ Merge â†’ Validate Merged â†’ Clean Merged â†’ 
Anomaly Detection â†’ Bias Detection â†’ Drift Detection â†’ 
DVC Versioning â†’ Testing â†’ Ready for Modeling


### *Dual-Pipeline Design:*

*Pipeline 1 (VAE - Scenario Generation):*
â€¢â   â Input: â â€¯macro_features.csvâ€¯â  (FRED + Market)
â€¢â   â Purpose: Generate stress test scenarios
â€¢â   â Shape: ~5,500 rows Ã— ~65 columns

*Pipeline 2 (XGBoost/LSTM - Prediction):*
â€¢â   â Input: â â€¯merged_features.csvâ€¯â  (FRED + Market + Company)
â€¢â   â Purpose: Predict company outcomes under scenarios
â€¢â   â Shape: ~188,000 rows Ã— ~133 columns

---

## ğŸ“ Project Structure

```text
Mlops_Project_FinancialCrises/
â”‚
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ requirements-test.txt               # Testing dependencies
â”œâ”€â”€ .env                                # Environment variables (DO NOT COMMIT)
â”œâ”€â”€ .env.example                        # Template for .env
â”œâ”€â”€ .gitignore                          # Git ignore rules
â”œâ”€â”€ pytest.ini                          # Pytest configuration
â”œâ”€â”€ dvc.yaml                            # DVC pipeline definition
â”œâ”€â”€ params.yaml                         # Pipeline parameters
â”œâ”€â”€ Makefile                            # Convenient commands
â”œâ”€â”€ docker-compose.yml                  # Airflow setup
â”‚
â”œâ”€â”€ data/                               # Data directory (tracked by DVC)
â”‚   â”œâ”€â”€ raw/                            # Raw data from APIs
â”‚   â”‚   â”œâ”€â”€ fred_raw.csv
â”‚   â”‚   â”œâ”€â”€ market_raw.csv
â”‚   â”‚   â”œâ”€â”€ company_prices_raw.csv
â”‚   â”‚   â”œâ”€â”€ company_income_raw.csv
â”‚   â”‚   â””â”€â”€ company_balance_raw.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ clean/                          # Cleaned data (PIT correct)
â”‚   â”‚   â”œâ”€â”€ fred_clean.csv
â”‚   â”‚   â”œâ”€â”€ market_clean.csv
â”‚   â”‚   â”œâ”€â”€ company_prices_clean.csv
â”‚   â”‚   â”œâ”€â”€ company_balance_clean.csv
â”‚   â”‚   â””â”€â”€ company_income_clean.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ features/                       # Feature-engineered data
â”‚   â”‚   â”œâ”€â”€ fred_features.csv
â”‚   â”‚   â”œâ”€â”€ market_features.csv
â”‚   â”‚   â”œâ”€â”€ company_features.csv
â”‚   â”‚   â”œâ”€â”€ macro_features.csv          # Pipeline 1
â”‚   â”‚   â”œâ”€â”€ merged_features.csv         # Pipeline 2
â”‚   â”‚   â”œâ”€â”€ macro_features_clean.csv
â”‚   â”‚   â””â”€â”€ merged_features_clean.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ reports/                        # Cleaning & validation reports
â”‚   â”œâ”€â”€ validation_reports/             # GE validation results
â”‚   â”œâ”€â”€ anomaly_reports/                # Anomaly detection results
â”‚   â”œâ”€â”€ bias_reports/                   # Bias detection results
â”‚   â””â”€â”€ drift_reports/                  # Drift detection results
â”‚
â”œâ”€â”€ src/                                # Source code
â”‚   â”œâ”€â”€ validation/                     # Validation scripts
â”‚   â”‚   â”œâ”€â”€ robust_validator.py
â”‚   â”‚   â”œâ”€â”€ validation_utils.py
â”‚   â”‚   â”œâ”€â”€ ge_validator_base.py
â”‚   â”‚   â”œâ”€â”€ validate_checkpoint_1_raw.py
â”‚   â”‚   â”œâ”€â”€ validate_checkpoint_2_clean.py
â”‚   â”‚   â”œâ”€â”€ validate_checkpoint_3_merged.py
â”‚   â”‚   â”œâ”€â”€ validate_checkpoint_4_clean_merged.py
â”‚   â”‚   â””â”€â”€ step5_anomaly_detection.py
â”‚   â”‚
â”‚   â”œâ”€â”€ monitoring/                     # Monitoring & alerting
â”‚   â”‚   â””â”€â”€ alerting.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/                          # Utilities
â”‚       â”œâ”€â”€ config.py                   # Configuration loader
â”‚       â””â”€â”€ dvc_helper.py               # DVC utilities
â”‚
â”œâ”€â”€ dags/                               # Airflow DAGs
â”‚   â””â”€â”€ financial_crisis_pipeline_dag.py
â”‚
â”œâ”€â”€ tests/                              # Test suite (84% coverage)
â”‚   â”œâ”€â”€ conftest.py                     # Shared fixtures
â”‚   â”œâ”€â”€ test_data_collection.py
â”‚   â”œâ”€â”€ test_data_cleaning.py
â”‚   â”œâ”€â”€ test_feature_engineering.py
â”‚   â”œâ”€â”€ test_data_merging.py
â”‚   â”œâ”€â”€ test_post_merge_cleaning.py
â”‚   â”œâ”€â”€ test_anomaly_detection.py
â”‚   â”œâ”€â”€ test_bias_detection.py
â”‚   â”œâ”€â”€ test_drift_detection.py
â”‚   â”œâ”€â”€ test_validation.py
â”‚   â”œâ”€â”€ test_config.py
â”‚   â””â”€â”€ test_utils.py
â”‚
â”œâ”€â”€ logs/                               # Pipeline logs
â”‚   â””â”€â”€ pipeline_metrics.json
â”‚
â”œâ”€â”€ great_expectations/                 # GE configuration
â”‚   â””â”€â”€ great_expectations.yml
â”‚
â””â”€â”€ Pipeline Scripts (Root Directory)
    â”œâ”€â”€ step0_data_collection.py
    â”œâ”€â”€ step1_data_cleaning.py
    â”œâ”€â”€ step2_feature_engineering.py
    â”œâ”€â”€ step3_data_merging.py
    â”œâ”€â”€ step4_post_merge_cleaning.py
    â”œâ”€â”€ step5_bias_detection_with_explicit_slicing.py
    â””â”€â”€ step6_anomaly_detection
    â””â”€â”€ step7_drift_detection.py
```

---

## âœ… Prerequisites

### *Required Software:*
â€¢â   â Python 3.9+
â€¢â   â Git
â€¢â   â Docker & Docker Compose (for Airflow)
â€¢â   â pip (Python package manager)

### *Required Accounts (Free):*
â€¢â   â *Alpha Vantage API* - [Get free key](https://www.alphavantage.co/support/#api-key)
â€¢â   â *FRED API* (optional) - [Get free key](https://fred.stlouisfed.org/docs/api/api_key.html)
â€¢â   â *Slack Webhook* (optional) - [Setup guide](https://api.slack.com/messaging/webhooks)
â€¢â   â *Gmail App Password* (optional) - [Setup guide](https://support.google.com/accounts/answer/185833)

---

## ğŸš€ Installation

### *Step 1: Clone Repository*

â â€¯bash
git clone https://github.com/yourusername/Mlops_Project_FinancialCrises.git
cd Mlops_Project_FinancialCrises
â€¯â 

### *Step 2: Create Virtual Environment*

â â€¯bash
# Create virtual environment
python3 -m venv fenv

# Activate (Mac/Linux)
source fenv/bin/activate

# Activate (Windows)
fenv\Scripts\activate
â€¯â 

### *Step 3: Install Dependencies*

â â€¯bash
# Install main dependencies
pip install --upgrade pip
pip install -r requirements.txt

# Install test dependencies
pip install -r requirements-test.txt
â€¯â 

### *Step 4: Initialize DVC*

â â€¯bash
# Initialize DVC
dvc init

# Add remote storage (choose one):

# Option A: Local remote (for testing)
dvc remote add -d local_remote /tmp/dvc-storage

# Option B: S3 remote (for production)
dvc remote add -d s3remote s3://your-bucket/dvc-storage
dvc remote modify s3remote access_key_id YOUR_AWS_KEY
dvc remote modify s3remote secret_access_key YOUR_AWS_SECRET

# Option C: Google Drive (free)
dvc remote add -d gdrive gdrive://YOUR_FOLDER_ID
â€¯â 

### *Step 5: Setup Great Expectations*

â â€¯bash
# Initialize Great Expectations (will be done automatically by validation scripts)
# Or manually:
great-expectations init
â€¯â 

---

## âš™ï¸ Configuration

### *Create .env File*

Copy the template and fill in your values:

â â€¯bash
# Copy template
cp .env.example .env

# Edit with your values
nano .env  # or use your favorite editor
â€¯â 

### *Minimal .env Configuration:*

â â€¯bash
# ============================================================================
# REQUIRED CONFIGURATION
# ============================================================================

# Airflow
AIRFLOW_UID=50000
AIRFLOW__CORE__FERNET_KEY=YOUR_FERNET_KEY_HERE
AIRFLOW__WEBSERVER__SECRET_KEY=YOUR_SECRET_KEY_HERE

# API Keys
ALPHA_VANTAGE_API_KEY=YOUR_ALPHA_VANTAGE_KEY_HERE

# DVC
DVC_REMOTE_TYPE=local
DVC_LOCAL_REMOTE=/tmp/dvc-storage

# Pipeline Parameters
START_DATE=2005-01-01
END_DATE=today
REPORTING_LAG_DAYS=45

# ============================================================================
# OPTIONAL CONFIGURATION
# ============================================================================

# Alerts (can disable)
SLACK_ALERTS_ENABLED=false
EMAIL_ALERTS_ENABLED=false

# Thresholds
ANOMALY_IQR_THRESHOLD=3.0
BIAS_REPRESENTATION_THRESHOLD=0.3
MAX_MISSING_PCT_CLEAN=5
â€¯â 

### *Generate Required Keys:*

â â€¯bash
# Generate Fernet key for Airflow
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"

# Generate secret key for Airflow
python -c "import secrets; print(secrets.token_urlsafe(32))"
â€¯â 

---

## ğŸ® Pipeline Execution

### *Option 1: Run Complete Pipeline (Recommended)*

â â€¯bash
# Make run script executable
chmod +x run_pipeline.sh

# Run complete pipeline
./run_pipeline.sh
â€¯â 

### *Option 2: Run Individual Steps*

â â€¯bash
# Step 0: Data Collection (~18 minutes)
python step0_data_collection.py

# Checkpoint 1: Validate Raw Data
python src/validation/validate_checkpoint_1_raw.py

# Step 1: Data Cleaning
python step1_data_cleaning.py

# Checkpoint 2: Validate Clean Data
python src/validation/validate_checkpoint_2_clean.py

# Step 2: Feature Engineering
python step2_feature_engineering.py

# Step 3: Data Merging
python step3_data_merging.py

# Checkpoint 3: Validate Merged Data
python src/validation/validate_checkpoint_3_merged.py

# Step 3c: Clean Merged Data
python step3c_post_merge_cleaning.py

# Step 5: Anomaly Detection
python src/validation/step5_anomaly_detection.py

# Step 4: Bias Detection
python step4_bias_detection.py

# Step 6: Drift Detection
python step6_drift_detection.py

# DVC: Version all data
dvc add data/raw data/clean data/features
dvc push
git add data/*.dvc .dvc/config
git commit -m "Pipeline run complete"
git push
â€¯â 

### *Option 3: Run with Airflow (Production)*

â â€¯bash
# Step 1: Start Airflow
docker-compose up -d

# Step 2: Wait for initialization (~2 minutes)
docker-compose logs -f airflow-init

# Step 3: Access Airflow UI
# Open: http://localhost:8080
# Username: admin
# Password: admin

# Step 4: Trigger DAG from UI or CLI
docker-compose exec airflow-webserver airflow dags trigger financial_crisis_detection_pipeline

# Step 5: Monitor progress in UI
# View logs, task status, and execution graph
â€¯â 

### *Option 4: Run with DVC Pipeline*

â â€¯bash
# Run entire DVC pipeline
dvc repro

# Run specific stage
dvc repro data_cleaning

# Check pipeline status
dvc status
â€¯â 

---

## ğŸ” Data Validation

### *Validation Checkpoints:*

The pipeline includes 4 validation checkpoints using Great Expectations:

*Checkpoint 1: Raw Data Validation*
â€¢â   â Files exist
â€¢â   â Required columns present
â€¢â   â Data types correct
â€¢â   â Reasonable row counts
â€¢â   â Date ranges valid

*Checkpoint 2: Clean Data Validation*
â€¢â   â Missing values handled (<5%)
â€¢â   â No inf values
â€¢â   â Duplicates removed
â€¢â   â Point-in-time correctness maintained
â€¢â   â Forward-fill only (no look-ahead bias)

*Checkpoint 3: Merged Data Validation*
â€¢â   â Merge quality (no data loss)
â€¢â   â Date alignment correct
â€¢â   â All companies have macro context
â€¢â   â No duplicate (Date, Company) pairs

*Checkpoint 4: Clean Merged Data Validation*
â€¢â   â Zero inf values (CRITICAL)
â€¢â   â Minimal missing (<2%)
â€¢â   â No duplicate columns with suffixes
â€¢â   â Valid financial ratios
â€¢â   â Proper data types

### *View Validation Results:*

â â€¯bash
# View validation reports
ls -lh data/validation_reports/

# View specific report
cat data/validation_reports/ge_merged_features_clean_*.json | python -m json.tool

# View Great Expectations data docs
great-expectations docs build
# Then open: great_expectations/uncommitted/data_docs/local_site/index.html
â€¯â 

---

## ğŸ§ª Testing

### *Run Tests:*

â â€¯bash
# Run all tests with coverage
pytest --cov=src --cov=. --cov-report=html --cov-report=term-missing

# Or use Makefile
make test          # Run all tests
make coverage      # Run with coverage report

# Or use helper script
./run_tests.sh
â€¯â 

### *Test Coverage:*

Current coverage: *84%* (exceeds 75% requirement)

â â€¯bash
# View coverage report
open htmlcov/index.html  # Mac
xdg-open htmlcov/index.html  # Linux
â€¯â 

### *Run Specific Tests:*

â â€¯bash
# Test specific module
pytest tests/test_data_cleaning.py -v

# Test specific class
pytest tests/test_drift_detection.py::TestKSTestDriftDetection -v

# Run fast tests only
pytest -m "not slow"

# Run in parallel
pytest -n auto
â€¯â 

---

## ğŸ“Š Monitoring & Alerts

### *Anomaly Detection:*

Detects three types of anomalies:
1.â  â *Statistical outliers* - IQR method (>3 std deviations)
2.â  â *Business rule violations* - Domain-specific rules (e.g., negative VIX)
3.â  â *Temporal anomalies* - Sudden jumps (>50% change)

*Output:*
â€¢â   â Flag columns added (no data modification)
â€¢â   â Detailed reports in â â€¯data/anomaly_reports/â€¯â 
â€¢â   â Alerts sent if critical anomalies found

### *Bias Detection:*

Performs data slicing across 3 dimensions:
1.â  â *Company-level* - 25 companies
2.â  â *Sector-level* - 9 sectors
3.â  â *Temporal* - 5 time periods (pre-crisis, crisis, recovery, COVID, recent)

*Output:*
â€¢â   â Slice statistics in â â€¯data/bias_reports/â€¯â 
â€¢â   â Mitigation recommendations
â€¢â   â Representation bias analysis

### *Drift Detection:*

Compares historical distributions:
â€¢â   â *Reference period:* 2005-2010
â€¢â   â *Current period:* 2020-2025
â€¢â   â *Method:* Kolmogorov-Smirnov test

*Output:*
â€¢â   â Drifted features report in â â€¯data/drift_reports/â€¯â 
â€¢â   â Feature stability analysis
â€¢â   â Mitigation recommendations

### *Alert Configuration:*

Alerts are sent when:
â€¢â   â Validation checkpoint fails
â€¢â   â Critical anomalies detected (>10 critical issues)
â€¢â   â High drift detected (>20 features with p<0.01)
â€¢â   â Pipeline step exceeds time threshold (>1 hour)

*Enable Alerts in .env:*

â â€¯bash
# Slack
SLACK_ALERTS_ENABLED=true
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL

# Email
EMAIL_ALERTS_ENABLED=true
SMTP_SERVER=smtp.gmail.com
SMTP_PORT=587
SENDER_EMAIL=your-email@gmail.com
SENDER_APP_PASSWORD=your_16_char_app_password
RECIPIENT_EMAILS=team@example.com,member@example.com
â€¯â 

---

## ğŸ”„ Reproducibility

### *Pull Data from DVC:*

â â€¯bash
# Pull all versioned data
dvc pull

# Pull specific stage
dvc pull data/raw.dvc
dvc pull data/clean.dvc
â€¯â 

### *Recreate Entire Pipeline:*

â â€¯bash
# Step 1: Clone repository
git clone https://github.com/yourusername/Mlops_Project_FinancialCrises.git
cd Mlops_Project_FinancialCrises

# Step 2: Setup environment
python3 -m venv fenv
source fenv/bin/activate
pip install -r requirements.txt

# Step 3: Configure
cp .env.example .env
# Edit .env with your API keys

# Step 4: Initialize DVC
dvc init
dvc remote add -d local_remote /tmp/dvc-storage

# Step 5: Pull data (if already versioned)
dvc pull

# Step 6: Or regenerate from scratch
python step0_data_collection.py
# ... run all steps

# Step 7: Run tests
pytest --cov=src --cov-report=html

# Step 8: Everything should match original results!
â€¯â 

### *Version Control Workflow:*

â â€¯bash
# After running pipeline successfully:

# 1. Version data with DVC
dvc add data/raw data/clean data/features
dvc push

# 2. Commit code and DVC files to Git
git add .
git commit -m "Pipeline run: $(date +%Y-%m-%d)"
git push

# 3. Tag release
git tag -a v1.0 -m "Validated pipeline with 84% test coverage"
git push --tags
â€¯â 

---

## ğŸ¯ Key Features

### *1. Point-in-Time Correctness âœ…*

Ensures no look-ahead bias:

â â€¯python
# Quarterly financials shifted +45 days
# Q1 2020 (Mar 31) â†’ Available May 15 (45 days later)

# Forward fill ONLY (no backward fill)
# Missing values filled from past, never from future
â€¯â 

*Why it matters:* Prevents using future information that wouldn't be available at prediction time.

### *2. Data Quality Assurance âœ…*

*4 Validation Checkpoints:*
â€¢â   â Raw data validation (schema, ranges, completeness)
â€¢â   â Clean data validation (missing <5%, no inf, no duplicates)
â€¢â   â Merged data validation (alignment, no data loss)
â€¢â   â Final validation (production-ready quality)

*Quality Metrics:*
â€¢â   â Zero inf values after cleaning
â€¢â   â <2% missing values in merged data
â€¢â   â No duplicate columns from merge operations
â€¢â   â Valid financial ratios

### *3. Bias Detection & Mitigation âœ…*

*Data Slicing Analysis:*
â€¢â   â 25 company slices
â€¢â   â 9 sector slices
â€¢â   â 5 temporal slices

*Biases Detected:*
â€¢â   â Representation bias (sample imbalance)
â€¢â   â Distribution bias (feature behavior across groups)
â€¢â   â Temporal bias (data quality over time)

*Mitigation Applied:*
â€¢â   â Stratified train/test split by Sector
â€¢â   â Weighted loss function (weight âˆ 1/company_samples)
â€¢â   â Crisis periods included in validation set

### *4. Anomaly Detection âœ…*

*Detection Methods:*
â€¢â   â IQR outliers (statistical)
â€¢â   â Business rule violations (domain-specific)
â€¢â   â Temporal jumps (time-series)

*Crisis Awareness:*
â€¢â   â Distinguishes 2008-2009 and 2020 outliers (valid) from data errors
â€¢â   â Flags but doesn't remove crisis data

*Output:*
â€¢â   â 16 flag columns added (e.g., â â€¯Stock_Return_1D_Outlier_Flagâ€¯â )
â€¢â   â Original data unchanged
â€¢â   â Detailed reports with severity levels

### *5. Comprehensive Testing âœ…*

*Test Coverage: 84%*

â â€¯bash
# Coverage breakdown:
src/validation/step5_anomaly_detection.py  86%
step1_data_cleaning.py                     83%
step3c_post_merge_cleaning.py              85%
step6_drift_detection.py                   85%
step4_bias_detection.py                    85%
â€¯â 

*Test Suite:*
â€¢â   â 118 passing tests
â€¢â   â Unit tests (fixtures, mocking)
â€¢â   â Integration tests (end-to-end)
â€¢â   â Edge case testing

---

## ğŸ“ˆ Output Files

### *Raw Data (data/raw/):*
â€¢â   â â â€¯fred_raw.csvâ€¯â  - 5,571 rows Ã— 13 columns
â€¢â   â â â€¯market_raw.csvâ€¯â  - 5,238 rows Ã— 2 columns
â€¢â   â â â€¯company_prices_raw.csvâ€¯â  - 129,569 rows (25 companies)
â€¢â   â â â€¯company_income_raw.csvâ€¯â  - 2,016 quarters
â€¢â   â â â€¯company_balance_raw.csvâ€¯â  - 2,016 quarters

### *Clean Data (data/clean/):*
â€¢â   â Same files with PIT correction applied
â€¢â   â Missing values handled
â€¢â   â Duplicates removed
â€¢â   â Outliers flagged (not removed)

### *Features (data/features/):*
â€¢â   â â â€¯macro_features_clean.csvâ€¯â  - Pipeline 1 (5,571 Ã— 67)
â€¢â   â â â€¯merged_features_clean.csvâ€¯â  - Pipeline 2 (188,670 Ã— 133)
â€¢â   â â â€¯merged_features_clean_with_anomaly_flags.csvâ€¯â  - With anomaly flags (188,670 Ã— 149)

### *Reports:*
â€¢â   â Validation reports (JSON)
â€¢â   â Anomaly reports (JSON + CSV)
â€¢â   â Bias reports (JSON + CSV)
â€¢â   â Drift reports (JSON + CSV)
â€¢â   â Test coverage report (HTML)

---

## ğŸ”§ Troubleshooting

### *Common Issues:*

*Issue 1: "ModuleNotFoundError: No module named 'src'"*

â â€¯bash
# Solution: Add to PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:${PWD}"

# Or install as package
pip install -e .
â€¯â 

*Issue 2: "Alpha Vantage API limit reached"*

â â€¯bash
# Solution: Wait 60 seconds or use multiple keys
# Add to .env:
ALPHA_VANTAGE_API_KEY_BACKUP=SECOND_KEY_HERE
â€¯â 

*Issue 3: "DVC push failed"*

â â€¯bash
# Check remote configuration
dvc remote list

# Verify credentials
dvc remote modify local_remote --local access_key_id YOUR_KEY
â€¯â 

*Issue 4: "pytest: error: unrecognized arguments: --cov"*

â â€¯bash
# Install pytest-cov
pip install pytest-cov

# Or use alternative
coverage run -m pytest tests/
coverage html
â€¯â 

*Issue 5: "Great Expectations validation failed"*

â â€¯bash
# View detailed report
great-expectations docs build
# Open: great_expectations/uncommitted/data_docs/local_site/index.html

# Check which expectations failed
cat data/validation_reports/ge_*.json | python -m json.tool
â€¯â 

---

## ğŸ“š Documentation

### *Additional Resources:*

â€¢â   â [Great Expectations Docs](https://docs.greatexpectations.io/)
â€¢â   â [DVC Documentation](https://dvc.org/doc)
â€¢â   â [Airflow Documentation](https://airflow.apache.org/docs/)
â€¢â   â [Pytest Documentation](https://docs.pytest.org/)

### *Project Documentation:*

â â€¯bash
# Generate API documentation
pip install pdoc3
pdoc --html --output-dir docs src/

# View docs
open docs/src/index.html
â€¯â 

---

## ğŸ¤ Contributing

### *Development Workflow:*

â â€¯bash
# 1. Create feature branch
git checkout -b feature/your-feature

# 2. Make changes
# Edit code...

# 3. Run tests
pytest

# 4. Run pipeline
python step1_data_cleaning.py  # etc.

# 5. Commit changes
git add .
git commit -m "Add: your feature"

# 6. Push and create PR
git push origin feature/your-feature
â€¯â 

### *Code Quality:*

â â€¯bash
# Run linting
pylint src/ step*.py

# Format code
black src/ step*.py

# Type checking
mypy src/
â€¯â 

---

## ğŸ“ Pipeline Statistics

### *Data Volume:*

| Stage | Files | Total Rows | Total Size |
|-------|-------|-----------|-----------|
| Raw | 5 | 142,437 | 18.22 MB |
| Clean | 5 | 142,437 | 26.45 MB |
| Features | 3 | 199,699 | 175.89 MB |
| Final | 2 | 194,241 | 151.01 MB |

### *Execution Time:*

| Step | Duration | Notes |
|------|----------|-------|
| Data Collection | ~18 min | Alpha Vantage rate limits |
| Data Cleaning | ~2 min | All datasets |
| Feature Engineering | ~3 min | Including quarterlyâ†’daily |
| Data Merging | ~1 min | Both pipelines |
| Validation (all) | ~5 min | 4 checkpoints |
| Anomaly Detection | ~2 min | Flag creation |
| *Total* | *~31 min* | End-to-end |

---

## ğŸ“ Learning Outcomes

This pipeline demonstrates:

âœ… *MLOps Best Practices*
â€¢â   â Data versioning (DVC)
â€¢â   â Pipeline orchestration (Airflow)
â€¢â   â Automated validation (Great Expectations)
â€¢â   â Comprehensive testing (pytest, 84% coverage)
â€¢â   â Monitoring & alerting (Email, Slack)

âœ… *Data Engineering*
â€¢â   â Point-in-time correctness
â€¢â   â Quarterly to daily conversion
â€¢â   â Feature engineering (45+ features per dataset)
â€¢â   â Multi-source data merging

âœ… *Data Quality*
â€¢â   â Anomaly detection (flag-only, crisis-aware)
â€¢â   â Bias detection (data slicing across 3 dimensions)
â€¢â   â Drift detection (historical comparison)
â€¢â   â Validation at each stage

âœ… *Production Readiness*
â€¢â   â Error handling & retries
â€¢â   â Detailed logging
â€¢â   â Alert system
â€¢â   â Reproducibility (DVC + Docker)
â€¢â   â Documentation

---

## ğŸ“ Support

For questions or issues:

1.â  â Check [Troubleshooting](#troubleshooting) section
2.â  â Review validation reports in â â€¯data/validation_reports/â€¯â 
3.â  â Check logs in â â€¯logs/â€¯â  directory
4.â  â Open an issue on GitHub

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details

---

## ğŸ™ Acknowledgments

â€¢â   â *FRED* - Federal Reserve Economic Data
â€¢â   â *Yahoo Finance* - Market & stock price data
â€¢â   â *Alpha Vantage* - Company fundamentals
â€¢â   â *Great Expectations* - Data validation framework
â€¢â   â *DVC* - Data version control

---

## ğŸš€ Quick Start Summary

â â€¯bash
# 1. Clone and setup
git clone <repo-url>
cd Mlops_Project_FinancialCrises
python3 -m venv fenv && source fenv/bin/activate
pip install -r requirements.txt

# 2. Configure
cp .env.example .env
# Add your ALPHA_VANTAGE_API_KEY to .env

# 3. Initialize DVC
dvc init
dvc remote add -d local_remote /tmp/dvc-storage

# 4. Run pipeline
python step0_data_collection.py  # ~18 min
python src/validation/validate_checkpoint_1_raw.py
python step1_data_cleaning.py
python src/validation/validate_checkpoint_2_clean.py
python step2_feature_engineering.py
python step3_data_merging.py
python src/validation/validate_checkpoint_3_merged.py
python step3c_post_merge_cleaning.py
python src/validation/step5_anomaly_detection.py
python step4_bias_detection.py
python step6_drift_detection.py

# 5. Version data
dvc add data/raw data/clean data/features
dvc push

# 6. Run tests
pytest --cov=src --cov-report=html

# 7. Success! âœ…
# - Data in: data/features/merged_features_clean.csv
# - Coverage: 84%
# - All validations passed
â€¯â 

---

*Built with â¤ï¸ for MLOps Course - Financial Crisis Detection Project*

*Last Updated:* October 2025
